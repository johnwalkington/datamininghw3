---
title: "Data Mining HW3"
author: "John Walkington, Colin McNally, Karlo Vlahek"
date: "`r Sys.Date()`"
output: md_document
---
```{r setup, include = FALSE}
setwd("~/Desktop/upload/datamininghw3")

library(tidyverse)
library(mosaic)
library(FNN)
library(modelr)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(modelr)
library(ModelMetrics)
library(caret)
library(parallel)
library(ggthemes)
library(foreach)
library(gbm)
library(pdp)
library(ggmap)

greenbuildings = read.csv("greenbuildings.csv")
dengue = read.csv("dengue.csv")
CAhousing = read.csv("CAhousing.csv")

set.seed(11050)
```
# 1)  What Causes What?

## Question #1

The problem with just finding data from random cities on crime rates and active police officers is that the data is too vague to truly prove causation. As a data scientist just with crime rate data and police force data, finding whether increased police presences causes lower crime rates is impossible. Increased police presence is usually due to high amounts of crime and being able to detect if high police presence causes a lowering in crime, we would need more information than just crime rates and police presence. We would need to be able to isolate times when a police presence is larger than usual for other reasons than crime, this would make proving causation much easier.

## Question #2

Researchers from the University of Pennsylvania were able to isolate the effect of police presence and crime. They did this by observing crime in Washington D.C. when there was a "High Alert" for terrorism. A "High Alert" in Washington D.C. forces a larger police presence throughout the city when it occurs. As well this then isolates crime from the police presence as the increased police presence in the city has nothing to do with crime rates. Their result showed that on "High Alert" days crime rates dropped. 

## Question #3

The second column of the table shows that ridership on the Metro during "High Alerts" was not significantly affected. The researchers tested for this because tourists and many victims of crime could be affected by the "High Alert" and not go to work or be on the streets during a "High Alert". The researchers, though, found that ridership was not affected during a "High Alert", and therfore crime went down because of increased police presence and not lack of victims.

## Question #4

Table #4 from the researchers' paper aims specifically to separate out different areas to see if there was a decrease in crime across all of Washington D.C. or only in one specific area. The researchers decided to isolate out the National Mall and compare it to the rest of Washington D.C.. Their findings show that the National Mall has a significant decrease in crime while the rest of Washington D.C. has a small decrease in crime that is not statistically significant. From the podcast we know that the main place that security is buffed during a terrorist threat is at the National Mall. The large decrease in crime in the First District (The National Mall) but no real significant decrease in crime elsewhere almost tells us that specifically crime decreases where large police presences are and not so much where there is not a large increase. You can then use this information to tell you that placing more police specific areas with high crime will decrease crime. Rather than increasing your general level of policing across a city this data tells you that increasing policing in a specific precinct can help lower crime.

# 2) Tree modeling: dengue cases
```{r include = FALSE}
dengue$city = dengue$city %>% factor()
dengue$season = dengue$season %>% factor()

dengue.split = initial_split(dengue, prop = 0.8)
dengue.train = training(dengue.split)
dengue.test = testing(dengue.split)

#CART Tree Step 1: Grow it Out!
dengue.cart.tree = rpart(total_cases~ ., data = dengue.train,
                         control = rpart.control(cp = 0.0001, minsplit = 30))

#Take a gander at the CV plot
#The complexity plot seems to bottom out at around 0.044
plotcp(dengue.cart.tree, ylim=c(0.7,1.2))

#CART Tree Step 2: Prune it Back!
#We'll use the CV error that's within one standard deviation of the minimum CP
cp_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  cp_opt
}

# cp_1se(dengue.cart.tree)

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

dengue_prune = prune_1se(dengue.cart.tree)
```
```{r echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
rpart.plot(dengue_prune, type = 1, digits = 4, extra = 1 )
modelr::rmse(dengue.cart.tree, dengue.test)
```

When growing out the tree model and pruning it back, a complexity function that computes the complexity parameter within 1 standard deviation of the minimum cross-validated error is called. Another function is called which prunes the tree at this 1 standard deviation complexity level. Visualizing this pruned tree above and calculating the root mean squared error of about 42 dengue fever cases. We see the tree is indeed simplified. Trees that perform well tend to be deeper, and perhaps can allude to why this model performs the worst out of the three utilized here, but is easily interpreted at this level.

```{r include = FALSE}
#Let's do a random forest now!
dengue_forest = randomForest(total_cases ~ . ,data = dengue.train, na.action = na.roughfix, importance = TRUE)
```
```{r echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
varImpPlot(dengue_forest, main = 'Variable Importance Plot for Random Forest', type =1)
modelr::rmse(dengue_forest, dengue.test)
```


We can see above that seasonality, the "Normalized Difference Vegetation Index" for the Northwest, and specific_humidity are among the most important variables the random forest considers. This random forest model produces an RMSE of around 28 dengue fever cases. This plot is calculated by comparing the performance accuracy of the model with an original variable to the performance accuracy of a model that utilizes a permuted variable.

```{r include = FALSE}
#Finally, the boosted tree model
dengue_boost = gbm(total_cases ~ . , data = dengue.train, n.trees = 500, cv.folds = 6)
```
```{r echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
gbm.perf(dengue_boost)

modelr::rmse(dengue_boost, dengue.test)
```

Here, using a gradient boosted model, simple 'base' learners are combined through iterations to produce a final estimate. The graph above shows the performance metrics evolution as the gbm function combines more and more of these simple 'base' learners. The black line is the training Gaussian squared error loss (the distribution that is assumed by gbm as per the squared error loss on the y-axis), and the green line represents the testing Gaussian squared error loss. The tree amount selected for prediction (using 164 trees in this instance) is indicated by the blue, vertical, dashed line which indicates the point of where the testing error on the cross-validated folds is minimized.

Overall, it is observed that the random forest model produces the lowest RMSE time and time again. So, let's incorporate some partial dependence plots below. The three partial dependence plots below depict the marginal effect that the specified feature has on predicted total dengue fever cases.
```{r include=FALSE}
p1.gg = dengue_forest %>% 
  partial(pred.var = 'specific_humidity') %>% 
  autoplot(train = dengue.train, rug = TRUE, xlab = 'Specific Humidity (Kelvin)', ylab = 'Predicted Dengue Cases')+
  theme_light() +
  ggtitle("PD of Specific Humidity on Predicted Dengue Cases")

p2.gg = dengue_forest %>% 
  partial(pred.var = 'precipitation_amt') %>% 
  autoplot(train = dengue.train, rug = TRUE, xlab = 'Precipitation (mm)', ylab = 'Predicted Dengue Cases') +
  theme_light() +
  ggtitle("PD of Precipitation on Predicted Dengue Cases")

p3.gg = dengue_forest %>% 
  partial(pred.var = 'season', fill = season) %>% 
  autoplot(train = dengue.train, rug = TRUE, xlab = 'Season', ylab = 'Predicted Dengue Cases') +
  theme_light() +
  ggtitle("PD of Seasonality on Predicted Dengue Cases")
```
```{r echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
p1.gg
p2.gg
p3.gg
```

Above we observe marginal effects of each specified feature. When passing the 'rug' argument in the pdp function, the distribution of data can be observed which allows for a clearer interpretation of the regions in the feature space, and actually helps mitigate over-interpretation. As the first two graphs show, it is wise to tread lightly when discussing the marginal effects of the variables when looking at the regions that have rug marks which are sparse and not close together. This primarily occurs on the outer portions of the graphs. Rugs are only visible for the first two instances of the partial dependence plots. The final partial dependence plot is categorical and used for it's ranked importance from the previous varImpPlot visual. For the predicted dengue cases here, the Fall season shows greater effects than all other seasons, with winter being a far-behind second. 
```{r, include = FALSE}
greenbuildings = greenbuildings %>%
  mutate(revenue = Rent * (leasing_rate/100)) %>%
  drop_na()

revenue_split =  initial_split(greenbuildings, prop=0.8)
revenue_train = training(revenue_split)
revenue_test  = testing(revenue_split)
```
# 3)  Predictive model building: green certification

Let's first start with a gradient-boosted tree model to see if we can predict revenue accurately from our features:

For feature engineering, to create a revenue per square foot per year variable, I divided leasing_rate (originally a percentage) by 100 to turn it into a proportion.  I then multiplied that by Rent to create my variable "revenue."
```{r, include = FALSE}
revenue.boost = gbm(revenue ~ size + empl_gr + stories + age + renovated + class_a + class_b
             + green_rating + net + amenities + cd_total_07 + hd_total07 
             + total_dd_07 + Precipitation + Gas_Costs + Electricity_Costs
             + City_Market_Rent, data = revenue_train,
             interaction.depth=4, n.trees=500, shrinkage=.05)
```
```{r, echo = FALSE}
gbm.perf(revenue.boost)
```
Here we see squared error as a function of number of trees.  We will continue to use 500 trees as it produces the lowest error and is not too computationally intensive.

Let's create some partial dependence plots with our gradient-boosted tree.  First we'll look at size:
```{r echo = FALSE}
pdp::partial(revenue.boost, pred.var = 'size', n.trees=500, plot = TRUE, rug= TRUE)
```
This partial dependence plot gives us a rough estimate of the relationship between building size and revenue.  We see that on average, a building with a size of 2 million square feet brings in about $18 more per square foot per year than a building with a size of 1 million square feet. This is approximate because only 10 percent of the data is bigger than 2 million square feet.

Let's visualize the partial dependence on green_rating:
```{r, echo = FALSE}
pdp::partial(revenue.boost, pred.var = 'green_rating', n.trees=500, plot = TRUE)
```
This partial dependence plot gives us a rough estimate of the relationship between building green rating and revenue. We see that on average, a building that is "green rated" brings in about $0.80 more per square foot per year than a building with a size of 1 million square feet.  This is a significant increase that would create a lot of extra revenue for a large building.

Let's calculate our out-of-sample RMSE to see how accurate our gradient-boosted tree model is:
```{r, echo = FALSE}
yhat_test_gbm = predict(revenue.boost, revenue_test, n.trees=500, type='response')
(yhat_test_gbm - revenue_test$revenue)^2 %>% mean %>% sqrt
```
This is pretty good, but let's see if we can get lower error with a random forest:
```{r, include = FALSE}
revenue.forest = randomForest(revenue ~ size + empl_gr
                              + stories + age + renovated + class_a + class_b 
                              + green_rating + net + amenities + cd_total_07 + hd_total07
                              + total_dd_07 + Precipitation + Gas_Costs + Electricity_Costs  
                              + City_Market_Rent, data = revenue_train, importance = TRUE)
```
I fit a random forest to regress "revenue" on all the variables in the training set except for those that I used to create the "revenue" measure: "Rent" and "leasing_rate." I also used the collapsed "green_rating" variable in place of separate LEED and EnergyStar variables.
```{r, echo = FALSE}
plot(revenue.forest)
```
This plot shows out-of-bag MSE as a function of number of trees averaged for the forest (randomForest defaults to 500.)  We see that the error bottoms out after about 100 trees.
```{r, echo = FALSE}
vi = varImpPlot(revenue.forest, type=1)
```
This plot shows the relative importance of our feature variables in the random forest in determining revenue.  The x-axis measures percent increase in MSE if we were to omit a given variable on the y-axis.  Here we see that size, age, and stories are by far the most important for fitting the forest, and green_rating lands at the bottom of our plot as the least important (only leads to a 10 percent increase in MSE if omitted.)

```{r echo = FALSE}
partialPlot(revenue.forest, revenue_train, 'size', las=1)
```
This partial dependence plot gives us a rough estimate of the relationship between building size and revenue.  We see that on average, a building with a size of 2 million square feet brings in about $10 more per square foot per year than a building with a size of 1 million square feet.

Let's visualize the partial dependence on green_rating:
```{r echo = FALSE}
partialPlot(revenue.forest, revenue_train, 'green_rating', las=1)
```
This partial dependence plot gives us a rough estimate of the relationship between building green rating and revenue. We see that on average, a building that is "green rated" brings in about $0.35 more per square foot per year than a building with a size of 1 million square feet.  This is not a very large difference at the margin, but might make a big difference in total revenue for a big building.

Let's check the out-of-sample RMSE for our random forest:
```{r}
modelr::rmse(revenue.forest, revenue_test)
```
This is lower than the RMSE we got with gradient-boosted trees, so let's use the predictions of the random forest: the estimated partial effect of green rating on revenue is $0.35 per square foot per year.

# 4) Predictive model building: California housing
```{r, include = FALSE}
lm_housing_split= initial_split(CAhousing, prop=0.8)
lm_housing_train= training(lm_housing_split)
lm_housing_test= testing(lm_housing_split)

#Standardizing totalBedrooms totalRooms
lm_Xtrain=model.matrix(households~totalBedrooms+totalRooms-1,
                       data=lm_housing_train)
lm_Xtest=model.matrix(households~totalBedrooms+totalRooms-1,
                      data=lm_housing_test)

#Had to Append Columns This Way as My Append Was Not Working
ytrain_households=lm_housing_train$households
ytest_households=lm_housing_test$households
ytrain_longitude=lm_housing_train$longitude
ytest_longitude=lm_housing_test$longitude
ytrain_latitude=lm_housing_train$latitude
ytest_latitude=lm_housing_test$latitude
ytrain_housingMedianAge=lm_housing_train$housingMedianAge
ytest_housingMedianAge=lm_housing_test$housingMedianAge
ytrain_population=lm_housing_train$population
ytest_population=lm_housing_test$population
ytrain_medianIncome=lm_housing_train$medianIncome
ytest_medianIncome=lm_housing_test$medianIncome
ytrain_medianHouseValue=lm_housing_train$medianHouseValue
ytest_medianHouseValue=lm_housing_test$medianHouseValue
```
```{r, include = FALSE}
#Scaling the Data
scale_train=apply(lm_Xtrain,2,sd)
Xtilde_train=scale(lm_Xtrain,scale=scale_train)
Xtilde_test=scale(lm_Xtest,scale=scale_train)
Xtilde_test=data.frame(Xtilde_test)%>%
  mutate(longitude=c(ytest_longitude),latitude=c(ytest_latitude),
         housingMedianAge=c(ytest_housingMedianAge),
         population=c(ytest_population),households=c(ytest_households),
         medianIncome=c(ytest_medianIncome),
         medianHouseValue=c(ytest_medianHouseValue))
Xtilde_train=data.frame(Xtilde_train)%>%
  mutate(longitude=c(ytrain_longitude),latitude=c(ytrain_latitude),
         housingMedianAge=c(ytrain_housingMedianAge),
         population=c(ytrain_population),households=c(ytrain_households),
         medianIncome=c(ytrain_medianIncome),
         medianHouseValue=c(ytrain_medianHouseValue))
```
```{r, include = FALSE}
#Creating predictive model to estimate our Median House Value
estimate_housing=lm(medianHouseValue~population+medianIncome
                    +households+households*medianIncome
                    +housingMedianAge+housingMedianAge*households,
                    data=Xtilde_train)
```
```{r, echo = FALSE}
summary(estimate_housing)
```
```{r, include = FALSE}
#Creating Output for Out of Sample Accuracy with Fitted RMSE
yhat_test_estimate_housing = predict(estimate_housing, Xtilde_test)
estimate_housing_rmse=rmse(Xtilde_test$medianHouseValue, 
                           yhat_test_estimate_housing)
```
```{r, echo = FALSE}
estimate_housing_rmse
```
```{r, include = FALSE}
#Mutating in the Predicted Values and Residuals to Our Data Frame
Xtilde_train=Xtilde_train%>%
  mutate(fitted_value=c(predict(estimate_housing)),residuals=c(resid(estimate_housing)))%>%
  filter(fitted_value>0)
```
```{r, include = FALSE}
#Generating Map of California to Use for Plotting
california_outline <- c(left = -125, bottom = 32, right = -113, top = 42.5)
california=get_stamenmap(california_outline, 
                         zoom = 5, 
                         maptype = "toner-lite") %>% ggmap() 
```
```{r, include = FALSE}
#Plotting Original Data of Median House Values in California
original=california+geom_point(data=Xtilde_train,
                      aes(x=longitude,
                          y=latitude,
                          color=medianHouseValue))+
  scale_color_gradient(low="blue", high="red")+
                      xlab('Longitude')+
                      ylab('Latitude')+
  labs(color='Median House Value',
       title='Median House Values of Land-Tracts in California:
                      Unedited In-Sample Data'
       )
```
```{r, echo = FALSE}
original
```
```{r, include = FALSE}
#Plotting the Fitted Values from Our Model
fitted=california+geom_point(data=Xtilde_train,
                      aes(x=longitude,
                          y=latitude,
                          color=fitted_value))+
  scale_color_gradient(low="blue", high="red")+
  xlab('Longitude')+
  ylab('Latitude')+
  labs(color='Fitted Median House Value',
       title='Median House Values of Land-Tracts in California:
                Fitted Model Values In-Sample'
  )
```
```{r, echo = FALSE}
fitted
```
```{r, include = FALSE}
residuals=california+geom_point(data=Xtilde_train,
                             aes(x=longitude,
                                 y=latitude,
                                 color=residuals))+
  scale_color_gradient(low="blue", high="red")+
  xlab('Longitude')+
  ylab('Latitude')+
  labs(color='Residual of Median House Value',
       title='Median House Value Residuals of Land-Tracts in California:
                       Fitted Model Values In-Sample'
  )
```
```{r, echo = FALSE}
residuals
```

When modelling the data we wanted to find a way to best estimate the median home value in a tract of land in California. We decided to include population, median income for the tract, # of households, and median house age as our variables in the model. We also decided to include some interaction variables in the equation as well, such as an interaction between # of households and median house age. I wanted to include an interaction between these two variables because a small sample of houses in a tract can greatly skew the median house age. As well another interaction was included between median household income and households for the same reason mentioned previously. The linear model was relatively accurate generating a consistent R-squared value above 0.52. Also when changing out relationships and dropping and adding variables this model continuously had a lower out-of-sample RMSE value. It should be noted though that our model had some trouble modelling a few data points. When modelling, about 15 or so tracts mainly in cities exhibited odd behavior of negative housing values. This was mainly due to large populations of people, in low income neighborhoods, with relatively young median house ages. These tracts mainly were in cities, and were more than likely dense,low income neighborhoods that just experienced new development. These few plot points were kept in the analysis and linear modelling for the sake of accuracy, but when graphing they were dropped. These points were dropped in the graphs because they were such far outliers that they would skew the scale and make the graphs unreadable. In order to actually make the graphs informative and differentiable for a majority of the data in our sample, we decided to drop these few outliers. As you can see, though, our model does a good job mimicking the data. Our graph of fitted values and the graph of the original data match well, and the graph of our residuals shows a large grouping of data around zero.
